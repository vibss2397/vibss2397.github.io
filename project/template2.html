<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Stylish Portfolio - Start Bootstrap Template</title>

    <!-- Bootstrap Core CSS -->
    <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.carousel.css" />
    <!-- Custom Fonts --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.theme.default.css" />
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="../vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../css/stylish-portfolio.css" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css" integrity="sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz" crossorigin="anonymous">


</head>

<body id="page-top">

<div class="container-fluid w-100 mynav py-2">
    <h3 class="text-center">[VIBLOG]</h3>
</div>


    <div class="container-fluid h-100" style="background-color: rgba(27,28,30,0.02)">
        <div class="container h-100">
        <div class="row h-100 align-items-center ">

            <div class="col-7">
                <h1 class="content-section-heading mb-4">Path Finding Algorithms(greedy and heuristics)<h1>

                <div class="links">
                    <div class="link m-3 m-3 text-center">
                    <i class="fab fa-github  fa-2x"></i>
                    <br>
                    <p> Source Code</p>
                </div>
                    <div class="link m-3 m-3 text-center">
                        <i class="fas fa-laptop fa-2x"></i>
                        <br>
                        <p>Live Preview</p>
                    </div>
                    <div class="link m-3 text-center">
                        <i class="fas fa-coffee fa-2x"></i>
                        <br>
                        <p>120 Cups of coffee</p>
                    </div>
                </div>

                <div class="content">
                    <h5>Description</h5>
                    <p style="font-weight: lighter;font-size: 20px"> Nunc varius nunc id tellus eleifend, ut
                        accumsan urna iaculis. Sed commodo tincidunt augue eget dapibus. Proin lacinia scelerisque tellus. Integer hendrerit ornare finibus. Nullam sed nunc velit. Nam enim velit, blandit sed tristique at, dignissim a arcu.
                        Sed in augue ac nisl auctor pharetra. Etiam sit amet cons
                        equat velit, vitae luctus elit. Morbi non nibh vitae ligula feugiat lacinia ac id nisl. In in nisl malesuada, hendrerit velit sed,</p>
                </div>
            </div>
            <div class="col-5">
                <div class="card grad-card" style="width: 100% !important;"></div>
            </div>
        </div>
        </div>
    </div>
    <div class="container">
        <div class="row justify-content-center mt-4">
             <div class="col-lg-8 col-xl-7 col-md-10">
                <p class="text">
                 This project was an amalgamation of a lot of different ideas I was having. Initially, the scope for this was 
                 to create and active learning system which could be used for labelling audio data (spectograms). Using cats vs dogs was a
                 small scale project to see a proof-of-concept. But while i started working on this project, i came across boosting and saw it's clear advantages
                 as an algorithm for classification and i changed my aim a little bit to use boosting, specifically Adaboost as the learning algorithm
                 and a shallow 1-layered-NN as the weak learner.
                 </p> 
                 <h4>Introduction to Adaboost</h4>
                 <p class="text">
                 Adaboost, which is proposed by schapire and freund in 1996, is a pretty good algorithm which works on the methodology that the prediction
                 by a bunch of weak learners can be combined to make a strong one. Random forests, use a similar mechanism but in most of their methodology 
                 they use mechanisms such as vote-by-majority to make their decisions. In contrast, in Adaboost each of the learner can be considered to be 
                 learning important information to classify a subset of the data. And when all of these predictors are taken together, and a weighted sum of
                 predictions is taken for all of them, that prediction can be in many cases be stronger than a single strong classifier.
                 </p><br>
                 <h4>The Algorithm</h4>
                 <img src="../img/active/adaboost.png" class="img-fluid" width="60%">
                 <br>
                 <p class="text">
                 At a very high level overview, initially every sample is assigned a probability, and as the rounds of the program progress
                 classification is done for every data point, which is used to calculate an error value (epsilon)[0/1 error]. This epsilon 
                 is used to calculate an alpha value, which is the weight for that classifier at that round. After this round the probability 
                 assigned to each data point is updated and next round of training is started with these probabilities.
                 </p>
                 <h4>Wait, How do i use active learning with adaboost?</h4>
                 <p class="text">
                 Active learning is a semi supervised learning method where there is very little amount of training data.
                 We need to use that limited data to make predictions on a large unlabelled pool of data. On every iteration of 
                 active learning we consider only those examples where the confidence for our model's prediction is not good enough.
                 For a binary classification problem [0/1 classification], we chose those data points which have an output label of close to 0.5.
                 </p>
                 <br>
                 <p class="text">
                    In our algorithm we implement Query-by-Boosting for active learning on a binary classification task. The points which are chosen to be added to the labelled training set are chosen as the ones at which the weighted voting has the least margin which is defined as the difference between sum of alphas belonging to one class and alphas belonging to the second one. One thing to note about this strategy is that this is more robust as is any active learning strategy which involves a committee than uncertainty sampling because uncertainty sampling assumes that the predictions which are high are correctly labelled and develops a bias to these solutions, where as in boosting since it is using multiple learners in its decision making, a data point is only considered as correctly labelled when most of these learners agree that it is.
                 </p>
                 <h4>Dataset</h4>
                 <p class="text">
                    In our project, we use a Cats v/s Dogs Dataset. It is a binary classification problem. The dataset consisted of 25000 images of Cats and Dogs split into a training and testing set size of 22000 and 3000 respectively. The test images are just used to test  The images are of dimensions 64*64*3 signifying a 3 channel RGB image. We then converted these images to grayscale and downscaled them using an Anti-Aliasing Filter. (Helps in preserving the maximum image quality when downsizing).
                 </p>
                 <img src="../img/active/dog_cat.png" width="400">
                 <br>
                 <h4>Experiments</h4>
                 <p class="text">
                 We approached this problem by first developing an understanding about how hard this problem was to solve. We used a simple CNN network shown below :
                 <br>
                 <img src="../img/active/base_cnn.png" width="600">
                 This CNN was used as the baseline or an upper bound for accuracy we wanted to achieve. We also developed a weaker version of this CNN for training in AdaBoost. The architecture of that CNN is :
                 <br>
                 <img src="../img/active/shallow_cnn.png" width="400">
                 <br>
                 We further tried to handicap this learner by limiting the number of iterations to train it on. We tested different values of the number of epochs to train this on and we settled at the value of 10 epochs. We also wanted to check whether this problem could be solved by a linear classifier. If it turned out to be true there was no need for using any other learner, so we also trained AdaBoost using a linear classifier as the weak learner.
                <br>
                 These different learners were trained on the entire dataset to set a baseline as to how difficult the problem was as solved by different complexities of learners. Our results are shown in the figure below. As shown in this figure, these learners perform very well on our dataset.
                <img src="../img/active/acc_learn.PNG" width="400">
                <br>
                With these baselines set up, it was now time to test how our algorithm actually worked in the context of labelling data and doing it efficiently. AdaBoost with CNN as the learner was run for 30 iterations with an aggressive data saving policy (100 data/iteration), intermediate data saving policy(300 data/iteration) and a lenient strategy(500 data/iteration). Even though every operation in all the scripts are vectorized to make them as fast as possible, having 3 levels of iterations, being : running gradient descent on CNN, Running adaboost with multiple CNN’s, running each iteration of active learning having adaboost. I suppose one strategy to make it faster would be an early stopping policy on adaboost where it stops a few iterations after reaching 0 error. The error for different strategies is given by the figure below.
                <img src="../img/active/err_learn.PNG" width="400">
                <br>
                We see that an intermediate data-adding policy while saving data achieves similar accuracy as that of lenient data-adding one. We can also see that active learning policy with linear classifiers performs terribly, however the performance is the same as the one trained on entire data.
                <br>
                One important thing to note here is that the concept of training and testing set does not exist, because we are assuming that every data point that we have is unlabelled and the learner gets access to only a subset of this data. Also, since the learner is making predictions on the entire unlabelled data-set in every iteration of active learning, it cannot be considered as testing data, only a validation set.
                <br>
                A point of concern was about how much data was utilized by the algorithm. This is important because there’s no point in using this method if it is consuming large amounts of data. Table below shows data consumption for different strategies we used. 
                <table class="table">
                    <thead>
                      <tr>
                        <th scope="col">Learner</th>
                        <th scope="col">Acuracy</th>
                        <th scope="col">Data Points</th>
                        <th scope="col">% data usage</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <th scope="row">CNN</th>
                        <td>0.9932</td>
                        <td>22000</td>
                        <td>1</td>
                      </tr>
                      <tr>
                        <th scope="row">Active learner with CNN (500 samples added per iteration)</th>
                        <td>0.7379</td>
                        <td>15500</td>
                        <td>0.704</td>
                      </tr>
                      <tr>
                        <th scope="row">Active learner with CNN (300 samples added per iteration)</th>
                        <td>0.7236</td>
                        <td>9000</td>
                        <td>0.41</td>
                      </tr>
                      <tr>
                        <th scope="row">Active learner with CNN (100 samples added per iteration)</th>
                        <td>0.6875</td>
                        <td>4000</td>
                        <td>0.18</td>
                      </tr>
                      <tr>
                        <th scope="row">Active learner with Linear</th>
                        <td>0.4956</td>
                        <td>4000</td>
                        <td>0.41</td>
                      </tr>
                      <tr>
                        <th scope="row">Active learning with random sampling with cnn</th>
                        <td>0.5526</td>
                        <td>9000</td>
                        <td>0.41</td>
                      </tr>
                    </tbody>
                  </table>
                 </p>
                <h4>Some Things learnt and future scope</h4>
                
                <ul class="list-group list-group-flush">
                    <li class="list-group-item"> Using ReLU with sigmoid is a fragile training proces, mostly outputs of sigmoid are saturated.</li>
                    <li class="list-group-item">Floating-point and 0/0 or Nan errors can be avoided by adding small numbers like 1e-8 to the softmax function.</li>
                    <li class="list-group-item">Shape Matching is very important while writing backprop especially, if doing mini-batch SGD.</li>
                    <li class="list-group-item">Use GPU optimization libraries whenever available.. (increase in speed in magnitude of atleast 10X)</li>
                    <li class="list-group-item">Check your data before feeding it to your learning algorithm.</li>
                  </ul>
            </div>
        </div>
    </div>

<footer class="footer text-center">
    <div class="container">
        <ul class="list-inline mb-3">
            <li class="list-inline-item">
                <a class="social-link rounded-circle text-white mr-3" href="#">
                    <i class="fab fa-facebook"></i>
                </a>
            </li>
            <li class="list-inline-item">
                <a class="social-link rounded-circle text-white mr-3" href="#">
                    <i class="fab fa-twitter"></i>
                </a>
            </li>
            <li class="list-inline-item">
                <a class="social-link rounded-circle text-white" href="#">
                    <i class="fab fa-github"></i>
                </a>
            </li>
        </ul>
        <p class="text-muted small mb-0">Copyright &copy; Vibhu Bhatia, Portfolio 2018</p>
    </div>
</footer>

<!-- Scroll to Top Button-->
<a class="scroll-to-top rounded js-scroll-trigger" href="#page-top">
    <i class="fas fa-angle-up"></i>
</a>

<!-- Bootstrap core JavaScript -->
<script src="../vendor/jquery/jquery.min.js"></script>
<script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

<!-- Plugin JavaScript -->
<script src="../vendor/jquery-easing/jquery.easing.min.js"></script>

<!-- Custom scripts for this template -->
<script src="../js/stylish-portfolio.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/owl.carousel.min.js"></script>


</body>

</html>
