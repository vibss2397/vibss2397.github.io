{
    "rnn": {
        "title": "charRNN(generating rick n morty subtitles via machine learning)",
        "abstract": "to be inserted",
        "image": "../img/gif/charRnn.gif",
        "body": "         <div class='row justify-content-center mt-4'> <div class='col-lg-8 col-xl-7 col-md-10'><b><h4>Where's My Episode:</h4></b><p class='text'>ewxcdies.<br>Huh?!<br>Rick, go home for a shit sanchez, Marty?<br>You gottanate-likes to tell you, and I ste favor me<br>Two... what?<br>Tophister wannanding he does.<br>Thying systemity, man.<br>All right, Morty.<br>Oh, get po back<br>a little univer?<br>Jesus...<br>What is thim chaze you to kill it.<br>You just personal space location.<br>Not gays.<br>Inco step it?<br>Uh, I'm [belches ]<br>Rick an tepho it.<br>On da exility unchreked usta?<br>What?!<br>Can they things to stupid...<br>They're just back to\nReall blowing up<br>and the plucks in your late alweal.<br>You kall Tommy. Don't have erowed interdagang overelcem.<br>Oh, the [Bleep]</p><br><p class='text-muted'>char rnn trained for 400 epochs on rick and morty subtitles</p><br> <p class='text'>I started hearing about this term 'machine learning', during my first year of college and was seeing organisations like google, facebook applying it almost everywhere so i considered it to be some kind of magic that only top notch programmers could apply. Little did i know, 2 years after that i would be applying these algorithms myself<br><br>Char rnn was one of the first projects i started working on which wasnt just me reading theory and the code and was kinda my starting point to code projects independently without just copy pasting the code and gloating when it worked (which it obviously would :P ). It was also one of the longest projects i worked on, partly because i didnt understand tensorflow that much, and the inspiration which came from this absolutely awesome <a href='https://gist.github.com/karpathy/d4dee566867f8291f086'>gist</a>  from andrej karpathy and initiated in me a genuine interest in deep learning, moving from idolizing Andrew ng to a wider community of researchers and engineers building different and amazing stuff.<br>        <br>If we look at it from a very Low Resolution perspective, an RNN seems too good to be true. Dealing with language, context matters, so as opposed to a traditional neural network we want to  preserve the previous input sequences in memory as opposed to independent variables in an ANN. Below i try to explain how the charRNN works and introduce an improvement- the LSTM. A better explanation and a deeper understanding can be developed by reading this post about <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness'>RNN</a>  and <a href='http://colah.github.io/posts/2015-08-Understanding-LSTMs/'>LSTM</a>.               <br>    <br></p><b><h4>charRnn-Background</h4></b>    <p class='text'>One of the main reasons why recurrent nets are used or even exist is due to the fact that other architectures like ann or cnn are too constrained in their inputs. instead rnn allows us to use sequences of inputs which can be found in real world in many cases be it natural language, image captioning or stocks where the current value is dependent on previous values. A rnn executes as it being unrolled in time steps. This fact is evident from the image below where the hidden layer for the first time step is fed to the second layer and so on. Another important thing to remember is that every time step within a <b>single layer</b> use <b>same weights</b>.                <br><br>                <img src='../img/charRnn/rnn.png' width='550'>                <br><br>                The term \u201cchar-rnn\u201d is short for \u201ccharacter recurrent neural network\u201d, and is effectively a recurrent neural network trained to predict the next character given a previous character. In this sense, we can think of a char-rnn as a classification model where we output a probability distribution of different classes when doing image classification, for a char-rnn we wish to output a probability distribution of character classes, i.e., a vocabulary of characters. In practice, each character is typically encoded as a one-hot vector, where the position of the one indicates the position of the character in the vocabulary. For example, in a vocabulary consisting of the twenty-six lower-case letters and space, any character would be represented as a 1 in that position and 0 in others. Below i show how a char rnn works.            <img src='../img/charRnn/charseq.jpg' width='450'>            </p>                <b><h4>Training a char-rnn using tensorflow:</h4></b><p class='text'>                    In his gist, andrej karpathy uses numpy to implement the char rnn and i wanted to implement this in tensorflow due to well faster training in gpu and well, tensorflow has such a neat way of code organisation where you define a model earlier and then use session to feed variables to this model. I was tired of seeing the same data being used everywhere,'the mini shakespeare', so a constant theme of this blog is that i use examples which are different than the traditional norm being used so that people can generalise the model for their purpose rather than having to read it from a variety of resources. In addition to that, most of my models are trained on jupyter notebook so they can get a cell by cell execution and understand how the algorithm really works. I trained 3 different datasets for my purpose - rick n morty subtitles (cuz why not :P), the generic  mini shakespeare and donald trump tweets ( cuz his tweets are interesting)                </p>                <b><h4>Learning from my mistakes :</h4></b>                    <ul>                      <li>track the initial state throughout and initialise it at the start of every epoch</li>                      <li>batch size and sequence size are 2 different things.(learnt the hardway)</li>                      <li>use LSTM instead of rnn because of remembering long term dependencies.</li>                    </ul>            </div>    </div>"
    },
    "path": {
        "title": "Path Finding Algorithms(greedy and heuristics)",
        "abstract": "to be inserted",
        "image": "../img/heuristics/comp.jpg",
        "body": "        <div class='row justify-content-center mt-4'>\n            <div class='col-lg-8 col-xl-7 col-md-10'>\n            \n            <p class='text'>\n                One of the coolest things about intelligence in machines is that even though we are not at a level where machines attain consciousness but the things we can code them to perform just absolutely blows my mind. During my DS and algo class i came across such an algorithm djikstra's algorithm, from the surface it is a pretty straight forward algorithm but just finding the path is not cool enough, i wanted to visualise how it worked and that's when i came across this awesome library in javascript P5.js. </p>\n<br><br>\n                <b><h4>About P5.js :</h4> </b><p class='text'>\n                p5.js is written in JavaScript but originally it is the library <a href='https://processing.org/'>Processing</a>. Processing is a visual coding tool for visual arts. The interesting is that processing is not only for developers but also artists, designers, researchers and those who want to enjoy making arts. p5.js is used in JavaScript so we can use it with DOM. Recently it started getting a lot of attention from developers and designers alike and i even used it to create this car chasing game(insert link) which interested me in going about this project and lo and behold, 1 day i decided to visualise djikstra algorithm along with another improvement i came across- the a* algorithm\n        <br>        <br>\n                </p>\n                <b><h4>Heuristics :</h4> </b><p class='text'>\n                A heuristic technique often called simply a heuristic, is any approach to problem solving, learning, or discovery that employs a practical method, not guaranteed to be optimal, perfect, logical, or rational, but instead sufficient for reaching an immediate goal. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. This is where our fundamental problem comes in- the path finding algorithms. Path finding requires algorithms to solve computational problems where we need to find the shortest distance between 2 points. This can be essential in many problems, such as solving mazes, building navigation systems and even in social networks to find the closest connections between any 2 people on the network\n                <br>    <br></p>\n                \n                <b><h4>Dijkstra's Algorithm</h4></b><p class='text'>\n                <img src='../img/gif/dijkstra.gif'>\n                For a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. This can be considered as a greedy algorithm as at every point in time it chooses the node with the combined distance of the previous node plus this one and keeps on continuing till it reaches the end or if no path is possible.\n                <br><br>\n                The reason why Dijkstra's algorithm works the way it does is in part because it exploits the fact that the shortest path between node u and w that includes point v also contains the shortest path from u to v and from v to w. If there existed something shorter between u to v, then it wouldn't be the shortest path\n\n                for coding this algorithm i directly used the steps in the pseudocode section of the <a href='https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm'>wikipedia page</a>. The code for this project can be found on the github link above. However, as we can see many of the paths it discovers in its quest for the shortest path are unnecessary, so it's far better than random guesses but still not good enough which is why we come across a better algorithm- a* (cool name tho).\n                \n            </p>\n                <b><h4>A-star algorithm</h4></b><p class='text'>\n                <img src='../img/gif/astar.gif'>\n                The major difference between a star and dijkstra is that the former takes more of an educated guess as opposed to exploring all the possibilities before finding the shortest one. \n\n                This can be explained by using a heuristic measure in addition to the distance of the node from the source. This heuristic can be the absolute distance between the current node and the destination, therefore, we can speed up the algorithm even more and avoid visiting nodes that increase the heuristic distance, which in this case is self evident if we look at both the gif here.\n\n                Thre pseudocode again was taken from <a href='https://en.wikipedia.org/wiki/A*_search_algorithm'>wikipedia page</a> and the code can be found on the github link.\n                </p>\n                <b><h4>Future Improvements :</h4></b>\n                    <ul>\n                      <li>make more mazes to convert this into kind of a game</li>\n                      <li>code this algorithm on a series of led lights controlled by either raspberry pi or arduino or some other microcontroller. (cuz it pretty)</li>\n                      <li>work on other cool algorithms. :P</li>\n                    </ul>    \n        </div>\n    </div>"
    },
    "qlearn": {
        "title": "qlearning(Playing frozen lake via qlearning)",
        "abstract": "to be inserted",
        "image": "../img/gif/qlearn.gif",
        "body": "        <div class=\"row justify-content-center mt-4\">\n            <div class=\"col-lg-8 col-xl-7 col-md-10\">\n            \n            <p class=\"text\">\n                This algorithm came into my knowledge when i just started into reinforcement learning and it just interested me when i read the algorithm and how such a simple algorithm could be so powerful.\n                </p>\n<br><br>\n                <b><h4>The algorithm :</h4> </b><p class=\"text\">\n                The goal of Q-Learning is to learn a policy, which tells an agent what action to take under what circumstances. Q-learning finds a policy that is optimal in the sense that it maximizes the expected value of the total reward over all successive steps, starting from the current state. \"Q\" names the function that returns the reward used to provide the reinforcement and can be said to stand for the \"quality\" of an action taken in a given state.\n\n        <br>        <br>\n                <img src=\"../img/qlearning/qlearnalgo.png\" width=\"700\"><br>\n                Using the above function, we get the values of Q for the cells in the table.When we start, all the values in the Q-table are zeros.There is an iterative process of updating the values. As we start to explore the environment, the Q-function gives us better and better approximations by continuously updating the Q-values in the table.<br>\n                <img src=\"../img/qlearning/qtable.png\" width=\"200\"><br>\n                \n                </p>\n                <b><h4>Frozen Lake :</h4> </b><p class=\"text\">\n                Open AI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Ping Pong. It provides various tasks for benchmarking our algorithms and is a pretty useful tool to simulate different tasks before loading them to the real world. \n \n                Frozen lake is a problem where given an array of different positions our main aim is to reach the destination (g) from the start(s). There are 2 types of intermidiate positions, either f or h where we are allowed to walk freely in the f blocks but h is a hole and if we land on it, the game is over.\n                After training the algorithm for 15000 episodes(an episode is over when agent dies), the algorithm starts producing quite reasonable guesses.\n\n                <br>    <br></p>\n                \n                <b><h4>Inspirations</h4></b><p class=\"text\">\n                My major interest in reinforcement learning just arose from the fact that in many cases i found myself to be stuck where i couldn't apply direct supervised algorithms but i knew that those problems could be solved. I found some cool applications of q learning in addition to the nature paper by google deepmind (https://www.nature.com/articles/nature14236) such as using deep q learning for improving free kicks in fifa (https://towardsdatascience.com/using-deep-q-learning-in-fifa-18-to-perfect-the-art-of-free-kicks-f2e4e979ee66), and playing doom using dqn (https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n                \n            </p>\n   \n        </div>\n    </div"
    },
    "shazam": {
        "title": "SeeThings, Shazam for things",
        "abstract": "\" what if i told you there is an app in the market\"",
        "image": "../img/seething/main.jpg",
        "body": "        <div class=\"row justify-content-center mt-4\">\n            <div class=\"col-lg-8 col-xl-7 col-md-10\">\n            \n            <p class=\"text\">\n                Silicon valley is one of the coolest tv show right now in the sense of its too acurate although a bit exaggerated portrayal of software developers.\n                <br><br>\n                <img src=\"../img/seething/jianyang.jpg\" width=\"400\"><br>\n                Particularly one of the most funniest episode was an episode where they create an app which uses machine learning to classify food, but our character jian yang builds a not-hotdog app which just classifies food as being hotdog or not hotdog. Jian yang has always been an odd character and kudos to jimmy o yang for portraying this but this inspired me to build\n                my own app that classifies things.<br>\n                <img src=\"../img/seething/not-hotdog.jpg\" width=\"400\">\n                <br><br>\n                Having worked with android studio for building a simple attendance and notes manager i always wanted to build an app on react-native which recently has turned out to be the go to method for building scalable apps fast so what better way to build an app that predicts things as my first app. I got inspiration for this by reading <a href=\"https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3\">this article</a> from the dev who made the app for the show.  \n\n                </p>\n<br><br>\n                <b><h4>SeeThings v1</h4> </b><p class=\"text\">\n                <img src=\"../img/seething/Screenshot_20181006-231102.jpg\" width=\"150\"><br>\n                Since i wanted a general purpose classification app so what better network to use than a pretrained network on a 1000 categories already. I had a vision for it in mind, for it to be as closely resembling to the not hotdog app but still having its originality. I used a pretrained MobileNet from keras and wanted to use it along with tensorflow js but tensorflow js wasnt supported for react native. Then, i moved on to using react native tensorflow, a wrapper around tensorflow lite module but it wasnt updated for the recent react native version and uppon recieving no updates from the community, i decided to move on to using a server based solution for solving this problem. \n\n        <br>        <br>\n                <img src=\"../img/seething/IMG-20181007-WA0002.jpg\" width=\"150\">\n                <img src=\"../img/seething/IMG-20181009-WA0003.jpg\" width=\"150\">\n                <img src=\"../img/seething/IMG-20181010-WA0001.jpg\" width=\"150\">\n                <img src=\"../img/seething/Screenshot_20181006-230950.jpg\" width=\"150\">\n                <img src=\"../img/seething/Screenshot_20181007-010241.jpg\" width=\"150\">\n                <br>\n                As of now v1 of the app uses server side hosting of the model on heroku to predict objets. which at times can be a cool game just to see what the model saw in the image to classify that object.\n                <br>\n                Edit : I recently, had a talk with Tim anglade, the developer of the app and he suggested that i make a javascript wrapper over tensorflow lite java implementation and use my model directly offline instead of server. So v1 will see quite a few changes in both overall working and ui.\n\n                <br>    <br></p>\n                \n                <b><h4>A Very quick Overview of how i made the app:</h4></b><br>\n                    <ul>\n                      <li>I initially decided for how the app was supposed to work, there were 4 screens i had in mind, the start screen, the camera screen, the result screen and the past results screen</li>\n                      <li>used a pretrained mobilenet model on keras and hosted on heroku using flask (will move on to a better server in v2 or offline support).</li>\n                      <li>initialised components for each screen and coded the associated behavior that every part is supposed to do</li>\n                      <li>used react-native-camera module to create a new camera screen with options like flash and swapping camera and auto zoom</li>\n                      <li>passed the image using base64 from client side and display the image along with the results on the result screen.</li>\n                      <li>wused the past results screen to store the previous results and share the results ( to be applied in v2)</li>\n                      <li>mapping emojis to all the categories( to be applied in v2) and displaying it along with the object class.</li>\n                      <li>wuse different models and get a good tradeoff between size and acuracy.</li>\n                    </ul>\n   \n        </div>\n    </div>"
    },
    "emoji": {
        "title": "Face2Emoji(classifying facial emotions using pretrained networks)",
        "abstract": "to be inserted",
        "image": "../img/gif/charRnn",
        "body": "        <div class=\"row justify-content-center mt-4\">\n            <div class=\"col-lg-8 col-xl-7 col-md-10\">\n            \n            <p class=\"text\">\n                The main goal of this project is to translate human emotions into a digital format. A pretrained xception model was used in this project where the last dense layers were removed and replaced by a custom layer for our prediction task. The Xception model was used because of its lower model size as opposed to VGG net but far better acuracy\n                \n\n                </p>\n<br><br>\n                <b><h4>Why Xception:</h4> </b><p class=\"text\">\n                I originally heard about the xception model in this <a href=\"https://youtu.be/0JWaSZZWTHM\">video by sentdex</a> where he develops an AI system for a self driving car in the game gta5. Right from thr start i was sold on the model due to 2 reasons:\n                <br> </p>\n                <ul>\n                    <li>Smaller model size than inception v3</li>\n                    <li>Far better acuracy than inception and other models with almost the same number of parameters.</li>\n                </ul>\n\t\t<p class=\"text\"\n                A quick search of xception gives the result \"Xception: Deep Learning with Depthwise Separable Convolutions\" and what it means is that if we first look at the inception hypothesis it states \u201ccross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly\u201d which means that we map an input layer to a smaller space using 1X1 convolution and then divide this smaller map into all of its channel and further perform seperate 3X3 convolutions on these channels.\n                \n                The Xception model takes this concept into extreme. Instead of partitioning input data into several compressed chunks, it maps the spatial correlations for each output channel separately, and then performs a 1\u00d71 depthwise convolution to capture cross-channel correlation.\n                <br> </p>\n                <img src=\"../img/face2emoji/xception.jpg\">\n                <br><br>\n                 <b><h4>The Dataset:</h4> </b>\n                <p class=\"text\">\n                    I decided to do this project when i was building a gesture recognition system but wasnt willing to use a large video dataset (cuz not a powerful gpu :P). The dataset i used for this task was the ck dataset which consists of 486 sequences of images from 96 people which created a total of 1000 images in total belonging to 8 emotions in total : <br>\n                        Anger, Disgust, Fear, Happy, Sadness, Surprise, Neutral and Contempt.\n                    <br><br></p>\n                    <img src=\"../img/face2emoji/1.png\" width=\"150\">\n                    <img src=\"../img/face2emoji/2.png\" width=\"150\">\n                    <img src=\"../img/face2emoji/3.png\" width=\"150\">\n                    <img src=\"../img/face2emoji/4.png\" width=\"150\">\n                    <br>\n                    <img src=\"../img/face2emoji/5.png\" width=\"150\">\n                    <img src=\"../img/face2emoji/6.png\" width=\"150\">\n                    <img src=\"../img/face2emoji/7.png\" width=\"150\">\n                    <img src=\"../img/face2emoji/8.png\" width=\"150\">\n                    <br>\n                     <b><h4>Training process:</h4> </b><p class=\"text\">\n                    The model was trained in keras with data augmentation for zoom and horizontal flipping and vertical rotation. The model was trained for about 1000 epochs using sgd optimizer and learning rate decay. In the end the model was stopped at training acuracy of about : 0.89 and validation acuracy of about: 0.60 which can be explained by 2 factors :<br> \n                    </p>    \n                <ul class=\"text\">\n                    <li>shortage of total number of training and validation samples</li>\n                    <li>redundancy in the data, since the data was a sequence of change of emotions on different subjects, the last 3 images for every emotion were considered to be belonging to that class and the first 2 images were considered to be of neutral category, so there wasn't a lot of variation in the images.<br>\n                    </ul>\n                    <br>\n                    <p class=\"text\">\n                    In addition to training i wrote a small script which generated real time detection using the webcam of the laptop and even though acuracy wasnt that high while training, the predictions were pretty acurate most of the time. \n                    <br>\n                    I thought of implementing a tensorflow js implementation for detection in the browser itself, but currently steered clear of that due to large size of the model (small, comparitively to other models) and not wanting a proper server to store and host the model just for one project.\n\n\n                </p>\n                \n   \n        </div>\n    </div>"
    },
    "gan": {
        "title": "Generative networks(generating superheroes using deep learning)",
        "abstract": "to be inserted",
        "image": "../img/gan/images.jpg",
        "body": "        <div class=\"row justify-content-center mt-4\">\n            <div class=\"col-lg-8 col-xl-7 col-md-10\">\n            \n            <p class=\"text\">\n                Every blog post i have come across regarding this topic always starts with this quote be it from open ai or medium posts so i figured might as well : <br>\n                \u201cWhat I cannot create, I do not understand.\u201d\n                                            \u2014Richard Feynman\n                <br><br>\n                Very recently, researchers at UC Berkley came up with <a href=\"https://arxiv.org/pdf/1808.07371v1.pdf\"> this paper </a>, appropriately titled, everybody dance now where machine can look up at any dance style and make the target person do that dancing style, which one can look at in <a href=\"https://youtu.be/PCBTZh41Ris\">this video</a>\n                <br><br>\n                Generative adverserial networks were something which i didnt even know did exist until i did and then was something that i saw literally everywhere in the deep learning community. Yann le cun, the director of facebook ai and research described gan as the most interesting idea in machine learning in the last 10 years and if we look at how they work it seems like something so simple yet so complex and filled with intricacies. We have 2 networks- generator and discriminator and we keep on training both of them till we reach the point where generator starts generating images which seem real to the discriminator.\n                <br><br>\n                As opposed to other architectures, i found this quite easy to implement, after i watched <a href=\"https://www.youtube.com/watch?v=HN9NRhm9waY\">this video of Ian Goodfellow</a> and taking a look at the model, i decided to implement my own version of gan using tensorflow eager, which was recently introduced by google, which is \"an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later\".\n                </p>\n<br><br>\n\n                <b><h4>Original or vanilla DCGAN(the prodigal son):</h4> </b><p class=\"text\">\n                <img src=\"../img/gif/dcgan.gif\"><br>\n                After viewing through a few implementations of dcgan, it became evident to me that MNIST had become the standard for evaluating a gan model, so i wanted to try a different dataset and lo and behold i found 1.\n                Being a dc fan it was clear in my mind i wanted to make a superhero generator but one quick view through dc comics website quickly proved it wasnt possible for dc superheroes, but marvel had a bunch of characters on their website (around 2000) so that became an ideal dataset that i used in my experiments.\n\n                <br></p>\n                <img src=\"../img/gan/marvel1.jpg\" width=\"150\">\n                <img src=\"../img/gan/marvel2.jpg\" width=\"150\">\n                <img src=\"../img/gan/marvel3.jpg\" width=\"150\">\n                <br><br>\n                <p class=\"text\">\n                   The architecture for dcgan is quite an easy one and using the <b>tf.keras.layers api</b> i was able to construct a model for both generator and discriminator quite easily and trained both the networks using the marvel dataset.\n                </p>\n                    <br><br>\n                    <img src=\"../img/gan/deep_convolutional_generative_adversarial_network1.png\" width=\"600\">\n                    <br>\n                    <p class=\"text\">\n                    The dcgan architecture uses sigmoid cross entropy loss for both the generator and discriminator\n                    which represents the kl divergence to measure the divergence of generated examples from real examples \n                    <br><br>\n                    The main goal of a generator in a gan is to keep on generating fake examples and passing them to a discriminator till it starts to detect them as real. The main goal of a discriminator is to not get fooled by a generator and keep on predicting whether the image it receives is fake or not.\n                    <br><br>\n                    during my training i also implemented some of the techniques to improve the results as described by <a href=\"https://blog.openai.com/generative-models/\">openAI</a>   which further helped in improving my results.\n                    <br><br>\n                    The training process was quite fast on a google colab notebook and each epoch took around 30 seconds, and i was able to see results quite quickly at around 600 epochs.\n\n                    </p>    \n                    <br><br>    \n                <b><h4>Wasserstein Gan(the stable one):</h4> </b><p class=\"text\">\n                <img src=\"../img/gif/wgan.gif\"><br>\n                when i read the algorithm for wgan, it seemed more natural to me as opposed to the vanilla dcgan, it can be because i understood its loss function, the earth mover distance quickly as opposed to the kl divergence function.\n\n                The training process was a bit stable than dcgan as it was less sensitive to changes in the learning rate, however the process took quite longer than i had expected and took a lot of iterations before giving me some observable results, i used the same architecture as dcgan but halved the number of kernels in every convolutional layer and the model still took longer(around 600 iterations) to give some observable results.\n\n                The wgan was preferred over vanilla dcgan basically due to 2 reasons:\n                </p>\n                <ul>\n                    <li>earth mover distance loss function provided a much stable training curve as opposed to the original implementation.</li>\n                    <li>weight clipping the critic's weights to between -0.01 and 0.01 reduced the value space for the weights and they were more tightly constrained and less prone for a very radical change.</li>\n                    <li>the critic was trained for more iterations per pass than the generator because  the critic should be trained to optimality at each step.</li>\n                </ul>\n               <b><h4>Present and Future work on gan:</h4> </b> \n                <ul>\n                    <li>implementing wgan-gp, which uses WGAN-GP penalizes the model if the gradient norm moves away from its target norm value 1(the prodigal son, part 2).</li>\n                    <li>generating drugs for diseases using a generative approach (currently working on this problem).</li>\n                </ul>\n        </div>\n    </div>"
    },
    "reinforce": {
        "title": "Policy gradients(teaching a bot to play atari games)",
        "abstract": "to be inserted",
        "image": "../img/gif/atari.gif",
        "body": "        <div class=\"row justify-content-center mt-4\">\n            <div class=\"col-lg-8 col-xl-7 col-md-10\">\n            \n            <p class=\"text\">\n                this algorithm, too was introduced to me by andrej karpathy, where in 90 lines he trained an agent which was able to play pong and even win against the atari game ai after sufficient training. The link to the post can be found <a href=\"http://karpathy.github.io/2016/05/31/rl/\">here</a>.\n                While researching for this algorithm, i came to a stop because i could understand the rationale behind it and why it worked but failed to find an implementation which worked on something other than pong, just a recreation of the original post. So, i decided to train an agent to play space invaders using policy gradients algorithm in tensorflow, and generalised it so that it could be used out of the box without much changes.\n                <br><br>\n                </p>\n\n        </div>\n    </div>"
    }
}